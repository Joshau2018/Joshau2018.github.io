<!DOCTYPE html>
<html lang="en">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    />
    <link
      href="https://fonts.googleapis.com/css?family=JetBrains Mono"
      rel="stylesheet"
    />
    <link href="/fancy.css" type="text/css" rel="stylesheet" />
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Joshua2018's site</title>
    <style>
      body {
        background-color: rgb(58, 59, 62);
        font-family: "Arial", sans-serif;
        margin: 20px;
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        padding: 20px;
      }

      .paper-card {
        background-color: #1b1a1a;
        border: 1px solid #dddddd00;
        padding: 15px;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(190, 178, 178, 0);
        max-height: 75vh; /* Set the maximum height as per your preference */
        max-width: 75vh;
        overflow-y: auto;
        margin: 20px auto; /* Center the container horizontally */
        display: flex;
        flex-direction: column;
        align-items: center;
      }

      h1,
      h2,
      p {
        margin-bottom: 20px;
      }
      img {
        display: block;
        margin-left: 1vh;
        margin-right: 1vh;
        max-width: 100%; /* Make sure images don't exceed their container's width */
      }
      .image-container {
        display: flex;
        justify-content: center; /* Center the images horizontally */
        align-items: center; /* Center the images vertically */
        flex-wrap: wrap; /* Allow the images to wrap to the next line on smaller screens */
        margin-top: 20px; /* Adjust the margin as needed */
      }
      .dark-mode {
        background-color: rgb(57, 53, 58);
        color: rgb(239, 236, 236);
      }

      .dark-mode a[href="/memoir.html"] {
        color: rgb(215, 61, 61);
      }

      .dark-mode-section-active {
        background-color: rgb(
          233,
          255,
          249
        ); /* Adjust the background color for the specific section */
        color: rgb(
          0,
          0,
          0
        ); /* Adjust the text color for the specific section */
      }
      .paper-card {
        margin-top: 20px; /* Adjust the value to your preference for spacing */
      }
      .sources {
        line-height: 1.5em;
        font-family: "JetBrains Mono";
        font-size: 1.75vh;
        margin-left: auto;
        margin-right: auto;
        max-width: 75vw; /* Set the maximum width as per your preference */
        text-align: left; /* Adjust text alignment as needed */
        text-indent: -1vh;
      }

      .sources a {
        color: darkgreen;
        text-decoration: underline;
        word-break: break-all;
        word-wrap: break-word; /* Allow long URLs to break onto the next line */
      }
    </style>
  </head>

  <body>
    <div class="topnav" id="myTopnav">
      <a href="/">Joshua's site</a>
      <div class="topnav" id="myTopnav" style="float: right">
        <a style="background-color: transparent; color: white">Argumentative</a>
        <a
          href="javascript:void(0);"
          class="mode-switch"
          onclick="darkModeToggle()"
        >
          <i class="fa fa-adjust"> </i> Toggle Mode
        </a>
        <a href="/webfolio.html">back</a>
        <a href="javascript:void(0);" class="icon" onclick="myFunction()">
          <i class="fa fa-bars"></i>
        </a>
      </div>
    </div>
    <div class="paper-card dark-mode-section-active">
      <h1
        style="
          line-height: 2em;
          font-family: 'JetBrains Mono';
          font-size: 2.75vh;
          text-align: center;
        "
      >
        Should There be Restrictions on AI (Artificial Intelligence) in warfare?
      </h1>
      <p
        style="
          line-height: 2.5em;
          text-indent: 3vh;
          font-family: 'JetBrains Mono';
          font-size: 2.5vh;
          margin-left: 3vh;
          margin-right: 3vh;
        "
      >
        It is hard to fathom that it has been over thirty years ago since the
        release of the movie “The Terminator.” Back then, the possibility of the
        events that conspired in the movie to actually happen in reality seemed
        implausible. However, that is not the case in current times as
        Artificial Intelligence (AI) is growing at an exceptional rate, because
        of this fact AI is becoming more common place in several career fields,
        such as the military. The military is already using AI for
        reconnaissance and is looking to expand the usage of AI. This raises the
        question: should there be restrictions on what AI can be used for
        warfare?
      </p>
      <p
        style="
          line-height: 2.5em;
          text-indent: 3vh;
          font-family: 'JetBrains Mono';
          font-size: 2.5vh;
          margin-left: 3vh;
          margin-right: 3vh;
        "
      >
        As mentioned, the military is already using AI to gather information on
        enemy forces' movements. In addition, the military is also using AI to
        defend against cyber-attacks while simultaneously going on the
        offensive. In addition, the military is trying to push for the creation
        of autonomous weapons, weapons that require no human interference acting
        by itself. The benefits that are leading them to this desire are reduced
        risk of the loss of human lives, advantage in technology, faster
        responses, and enhanced proficiency. However, there are concerns with
        the ethicality of the utilization of AI, the unpredictability of the
        actions of AI, and the risk of humans losing the ability to make
        military decisions. Thus, this creates the question of if the benefits
        outweigh the concerns.
      </p>
      <p
        style="
          line-height: 2.5em;
          text-indent: 3vh;
          font-family: 'JetBrains Mono';
          font-size: 2.5vh;
          margin-left: 3vh;
          margin-right: 3vh;
        "
      >
        The concerns of ethicality are important to figure out especially
        considering America’s ideology of always trying to be the good guys. The
        main concern of the ethicality of using AI this way is if this is a just
        utilization of AI or if warfare is something that only humans should be
        involved in? Then what if it is just and AI is used as a massive weapon
        would it stain the development of AI such as the nuclear bomb did with
        nuclear science. If we involve AI in warfare to create autonomous
        weapons, there will be a strain on the future development of AI
        similarly to how nuclear science shifted to focus on military
        applications. Thus, it will be best for the future development of AI if
        the usage of AI in warfare is limited.
      </p>
      <p
        style="
          line-height: 2.5em;
          text-indent: 3vh;
          font-family: 'JetBrains Mono';
          font-size: 2.5vh;
          margin-left: 3vh;
          margin-right: 3vh;
        "
      >
        In addition to ethical concerns there are several concerns with the
        effects AI would have on the ability for a human to make military
        decisions. Similarly, to how Nicholas Carr believes Google is negatively
        affecting our ability to think for ourselves, the International
        Committee of the Red Cross (ICRC) is concerned that AI will negatively
        affect our ability to make military decisions. As exemplified when they
        stated, “A negative impact on the quality of human decision-making in
        military settings,” as a risk of using Ai integrated systems. ICRC is
        concerned with the importance of humans to remain in control of AI. This
        is vividly expressed when the ICRC stated, “It is critical that the
        international community takes a genuinely human-centered approach to the
        development and use of AI in places affected by conflict.” They
        expressed the need for humans to control the growth of AI because once
        we lose control, we would lose the chance to control the damage AI
        deals. Similarly, to how in Jurassic World the dinosaurs escaped
        captivity causing the humans to no longer have control causing a lot of
        innocent people to die. Not only would AI be able to roam freely but
        humans would lose their ability to combat AI, so it is imperative that
        humans make military decisions not AI.
      </p>
      <p
        style="
          line-height: 2.5em;
          text-indent: 3vh;
          font-family: 'JetBrains Mono';
          font-size: 2.5vh;
          margin-left: 3vh;
          margin-right: 3vh;
        "
      >
        The biggest concern is the ability that if AI is used in autonomous
        weapons is that it could be the cause of a cataclysm. There is a theory
        called the paperclip theory which is about a super AI that can produce
        solutions on its own. The AI is tasked with the mundane task of making
        paper clips that goes rouge, because it learns to turn every metal into
        paper clips. Eventually, there will be human intervention, but the AI
        believes that they are in the way of its assignment. Thus, the AI
        produces the solution to simply kill all humans. Obviously, this is an
        issue considering that we are human and do not want to die, so to
        emplace an AI in a weapon designed to kill humans is already a red flag.
        Also, if AI is given the ability to launch nuclear war heads at its own
        discretion. It might send one if it believes that it is the only way to
        win the war causing a nuclear holocaust. AI is a scary thing that should
        be regulated heavily in warfare.
      </p>
      <p
        style="
          line-height: 2.5em;
          text-indent: 3vh;
          font-family: 'JetBrains Mono';
          font-size: 2.5vh;
          margin-left: 3vh;
          margin-right: 3vh;
        "
      >
        There are concerns but then there are also benefits about using AI in
        warfare. AI’s ability to make decisions at an expeditious pace is an
        important asset for the military. Considering that being just a second
        faster than the enemy could be the difference between life and death.
        There is also the fact that AI is more proficient than humans by AI not
        needing to rest or eat and has potential for higher accuracy. There is
        also the possibility of a decrease in the loss of human lives. This is
        plausible since most of the force fighting will be AI. These benefits
        are compelling, but are these benefits enough to need autonomous
        weapons?
      </p>
      <p
        style="
          line-height: 2.5em;
          text-indent: 3vh;
          font-family: 'JetBrains Mono';
          font-size: 2.5vh;
          margin-left: 3vh;
          margin-right: 3vh;
        "
      >
        The answer is no, those benefits do not warrant an attempt to create a
        weapon that has the potential of destroying the world. Yet, the usage of
        AI in warfare is already happening meaning eventually autonomous weapons
        will come into play. Thus, this creates the need for regulation on AI in
        warfare to protect public safety. Considering the need for regulation,
        there are already movements in Congress to ban the allowance of AI to
        launch nuclear weapons. There are also countries pushing for a legally
        binding treaty that will ban the making of autonomous weapons, yet none
        of the leading military powers are a part of it yet. The reason none of
        the leading powers joined yet is due to them believing they can get a
        one up on the other countries. There are people such as the late Stephen
        Harking, Elon Musk, and Bill Gates who are against the creation of AI
        without regulation, not just in warfare. Considering even the most
        influential people in technology fear the development of AI without
        regulation there is precedent for AI to have regulations in warfare.
      </p>
      <p
        style="
          line-height: 2.5em;
          text-indent: 3vh;
          font-family: 'JetBrains Mono';
          font-size: 2.5vh;
          margin-left: 3vh;
          margin-right: 3vh;
        "
      >
        In conclusion, AI offers several benefits in warfare, but if remained
        unregulated it might be the cause of humanity’s demise. There must be
        restrictions on AI to prevent the destruction of the world.
      </p>
      <p
        style="
          line-height: 2.5em;
          text-indent: 3vh;
          font-family: 'JetBrains Mono';
          font-size: 2.5vh;
          margin-left: 3vh;
          margin-right: 3vh;
        "
      ></p>
    </div>
    <div class="image-container"></div>
    <div class="paper-card dark-mode-section-active sources">
      Sources:
      <p>
        <a
          href="https://www.pbs.org/newshour/show/how-militaries-are-using-artificial-intelligence-on-and-off-the-battlefield"
          target="_blank"
          rel="noopener noreferrer"
          ><b
            >Rogin, Ali et al. How militaries are using artificial intelligence
            on and off the battlefield. PBS.org. 9 Jul. 2023.
            https://www.pbs.org/newshour/show/how-militaries-are-using-artificial-intelligence-on-and-off-the-battlefield.
          </b></a
        >
      </p>
      <p>
        <a
          href="https://cepr.org/voxeu/columns/ai-and-paperclip-problem"
          target="_blank"
          rel="noopener noreferrer"
        >
          Gans, Joshua. AI and the paperclip problem. Cepr.org. 10 Jun. 2018.
          https://cepr.org/voxeu/columns/ai-and-paperclip-problem.
        </a>
      </p>
      <p>
        <a
          href="https://www.theatlantic.com/magazine/archive/2008/07/is-google-making-us-stupid/306868/"
          target="_blank"
          rel="noopener noreferrer"
        >
          Carr, Nicholas. Is Google Making Us Stupid?. Theatlantic.com.
          July/August 2008.
          https://www.theatlantic.com/magazine/archive/2008/07/is-google-making-us-stupid/306868/.
        </a>
      </p>
      <p>
        <a
          href="https://www.icrc.org/en/document/what-you-need-know-about-artificial-intelligence-armed-conflict#:~:text=Armed%20forces%20are%20investing%20heavily,as%20part%20of%20weapon%20systems"
          target="_blank"
          rel="noopener noreferrer"
        >
          What you need to know about artificial intelligence in armed conflict.
          Icrc.org. 6 Oct. 2023.
          https://www.icrc.org/en/document/what-you-need-know-about-artificial-intelligence-armed-conflict#:~:text=Armed%20forces%20are%20investing%20heavily,as%20part%20of%20weapon%20systems.
        </a>
      </p>
    </div>

    <script>
      function darkModeToggle() {
        var element = document.body;
        File;
        element.classList.toggle("dark-mode");

        var sections = document.querySelectorAll(".paper-card");
        sections.forEach(function (section) {
          section.classList.toggle("dark-mode-section-active");
        });
      }
    </script>
    <script src="/navbar.js"></script>
  </body>
</html>
